---
title: 딥러닝의 들여다보기
tags: AIFFEL Fundamental DeepLearning
---

# 딥러닝 들여다보기 

### 신경망 구성의 개요 

**신경망(Neural Network)**이란 우리 뇌에는 `1000억 개`에 가까운 신경망 뉴런들이 서로 복잡하게 얽혀있어, 멀리서 보면 하나의 거대한 그물망 형태를 이루고 있고 이것을 **신경망**이라고 부른다. 

머신러닝/딥러닝 과학자들은 우리 뇌 속의 신경망 구조에 착안해서 **퍼셉트론(Perceptron)**이라는 형태를 제안하며 퍼셉트론을 연결한 형태를 인공신경망(Artificial Neural Network)라고 부르기 시작했다. 

<p align="center">
  <img src="/post_i/post2/post2_1.PNG">
</p>

위의 이미지는 총 3개의 레이어로 구성된 퍼셉트론(은닉층엔 `H개의 노드`, 출력층에는 `K개의 노드`가 존재하는 인공신경망, `+1`은 `bias`이기 때문에 이전 레이어와 연결 없음)

입력층(input layer), 은닉층(hidden layer), 출력층(output layer)로 이루어져 있다. (보통 입력층과 출력층 사이에 존재하는 층을 모두 은닉층이라고 부름)

그림으로 인공신경망을 표현할 땐, 노드 기준으로 3개의 레이어라고 할 수 있지만 실제로는 `2개의 레이어`를 가진다. (입력층-은닉층 사이, 은닉층-출력층 사이)

2개 이상의 레이어로 이루어지면 `다층 퍼셉트론(Multi-Layer Perceptron; MLP)`이라고 부르고 은닉층이 많을수록 인공신경망이 `Deep`해졌다고 표현한다. 



#### MINIST Revisited : 손글씨 숫자 이미지 데이터

예를 들어 입력값이 50개, 은닉 노드가 20개라면 입력층-은닉층 사이에는 `50x20`의 행렬이 존재하고 출력층의 노드가 10개라면 `20x10` 행렬을 가진다. 이 행렬들을 **Paraemter or Weight**라고 부른다. 

인접한 레이어 사이에는 아래와 같은 관계가 성립한다. 
$$
y=W \cdot X+b
$$

* MINIST data load

  ```python
  from tensorflow import keras
  mnist = keras.datasets.mnist
  (x_train, y_train), (x_test, y_test) = mnist.load_data()   
  
  x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0
  x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])
  x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])
  
  print('입력층 데이터의 모양 :', x_train_reshaped.shape)
  >>> (60000, 784)
  
  X = x_train_reshaped[:5]
  print('x_train_reshape 앞 5개 데이터 :', X.shape)
  >>> (5, 784)
  ```

* parameters

  ```python
  import numpy as np
  weight_init_std = 0.1
  input_size = 784
  hidden_size=50
  
  W1 = weight_init_std * np.random.randn(input_size, hidden_size) # W 생성 후 random 초기화 
  b1 = np.zeros(hidden_size) # bias 생성 후 zero로 초기화 
  a1 = np.dot(X, W1) + b1 # 은닉층 출력
  
  print(W1.shape)
  >>> (784, 50)
  print(b1.shape)
  >>>(50,)
  print(a1.shape)
  >>>(5, 50)
  ```

* 첫번째 데이터 은닉층 `a1` 출력 

  ```python 
  a1[0]
  >>> array([ 1.15167286,  0.3085079 ,  0.18968301, -0.7844581 ,  1.84049977,
          	  0.02554466,  0.09412182, -1.43044097,  0.62364172, -0.51576774,
      	      0.25043168,  0.75503683, -0.88530973,  1.83000312, -0.83884321,
    			    1.49657816,  1.36992041, -0.56558161,  1.73897263,  0.44173188,
   	         	0.16707376, -2.03277749, -0.22516795,  0.12933464,  0.49808142,
   	         -0.88022674, -0.64643659, -0.93729112,  0.78323751, -0.96854769,
   	         -2.25035599, -0.57268947,  1.03218086, -0.35055524,  0.68680353,
   	         -0.61970465, -0.36147022,  0.44210085,  0.07387209, -0.58357571,
   	         -0.14287283,  0.8380103 ,  0.30794246,  1.1120772 , -0.4310743 ,
   	       	  0.49420762,  0.00244365, -1.13121402,  0.78929279,  0.62201747])
  ```

  

### 활성화 함수와 손실 함수



 #### 활성화 함수(Activation Functions)

딥러닝에서 활성화 함수는 필수적으로 있어야 한다. 활성화 함수는 보통 비선형 함수를 사용해 모델의 표현력을 좋아지게 한다. 



1. **Sigmoid** 

  <img src="/post_i/post2/post2_2.PNG", width="5%">

  <p align="center">
   <img src="/post_i/post2/post2_3.PNG",width="50%">
  </p>


   Sigmoid 함수는 예전부터 많이 사용해왔지만, 현재는 **1. [vanishing gradient](https://brunch.co.kr/@chris-song/39), 2. not zero-centered, 3. exp 함수 사용시 발생하는 큰 비용**의 이유로 ReLU 함수를 더 많이 사용한다.

   * 은닉층 `a1`에 sigmoid 적용

     ```python
     def sigmoid(x)L
     	return 1 / (1 + np.exp(-x))
     
     z1 = sigmoid(a1)
     print(z1[0])
     >>> array([0.44499776, 0.51360289, 0.78594131, 0.55443348, 0.51541884,
          	   0.68635158, 0.34334359, 0.72814792, 0.29186884, 0.73683994,
          	   0.62554264, 0.80036752, 0.83492758, 0.38210999, 0.2367482 ,
          	   0.86645625, 0.81552476, 0.50701737, 0.44378242, 0.1785095 ,
          	   0.46142739, 0.54172233, 0.26654429, 0.32420165, 0.16328699,
          	   0.72918298, 0.53857095, 0.43677191, 0.88011371, 0.59563084,
          	   0.12320077, 0.3515041 , 0.42663484, 0.53308608, 0.82336197,
          	   0.78692182, 0.95233923, 0.64091891, 0.17398829, 0.509666  ,
          	   0.46761094, 0.73823935, 0.25650803, 0.40515   , 0.87166234,
          	   0.47198754, 0.6411463 , 0.33850205, 0.4204206 , 0.43542553]) # 0과 1사이의 값으로 출력
     ```



2. **Tanh**

   <img src="/post_i/post2/post2_4.PNG", width="5%">

   <p align="center">
    <img src="/post_i/post2/post2_5.PNG",width="50%">
   </p>


   tanh 함수는 중심값을 0으로 옮겨(zero-centered) sigmoid의 최적화 과정이 느려지는 문제를 해결했지만 여전히 vanishing gradient 문제가 존재한다. 



3. **ReLU** 

  <img src="/post_i/post2/post2_6.PNG", width="5%">

   <p align="center">
    <img src="/post_i/post2/post2_7.PNG",width="50%">
   </p>
   

   ReLU함수는 Sigmoid, Tanh 함수에 비해 **학습이 빠름, 연산비용이 크지 않고, 구현이 간단함**의 장점이 있다.

   

* MINIST MLP layer (Sigmoid layer + Dense layer)

  ```python
  def affine_layer_forward(X, W, b):
      y = np.dot(X, W) + b
      cache = (X, W, b)
      return y, cache
  
  input_size = 784
  hidden_size = 50
  output_size = 10
  
  W1 = weight_init_std * np.random.randn(input_size, hidden_size)
  b1 = np.zeros(hidden_size)
  W2 = weight_init_std * np.random.randn(hidden_size, output_size)
  b2 = np.zeros(output_size)
  
  a1, cache1 = affine_layer_forward(X, W1, b1)
  z1 = sigmoid(a1)
  a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됨
  
  print(a2[0])  # 최종 출력이 output_size만큼의 벡터
  >>> array([-0.37208949, -0.39140475,  0.36318281, -0.52107471,  0.2805353 ,
        	   -0.66172344,  0.32050707,  0.55887421, -0.47513079, -0.11513091])
  ```

* 최종 출력 `a2`에 softmax함수 적용 

  ```python
  def softmax(x):
      if x.ndim == 2:
          x = x.T
          x = x - np.max(x, axis=0)
          y = np.exp(x) / np.sum(np.exp(x), axis=0)
          return y.T 
  
      x = x - np.max(x) # 오버플로 대책
      return np.exp(x) / np.sum(np.exp(x))
  
  y_hat = softmax(a2)
  y_hat[0]  # 10개의 숫자 중 하나일 확률
  >>> array([0.1242319 , 0.13073301, 0.09501089, 0.06100132, 0.05761813,
             0.05451319, 0.06424347, 0.0794235 , 0.19205861, 0.14116599])
  ```

  

#### 손실 함수 (Loss Functions)

위의 과정처럼 비선형 활성화 함수가 있는 여러 은닉층을 거치고 신호들은 출력층으로 전달된다. 이 때, 우리가 원하는 정답과 전달된 신호들 사이의 차이를 줄이기 위해 각 파라미터들을 조정해야 한다. 이 차이를 구하는 데 사용되는 함수가 **손실함수(Loss function) 또는 비용함수(Cost function)**라고 부른다. 

1. **평균제곱오차(MSE:Mean Square Error)**
   $$
   MSE=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar Y_i)^2
   $$

2. **교차 엔트로피(Cross Entropy)**
   $$
   E=-\sum_{i=1}^nt_ilog\,y_i
   $$
   cross entropy는 두 확률분포 사이의 유사도가 클수록 작아진다. 학습이 많이 되지 않은 모델이 출력하는 softmax $\hat y$값은 10개의 숫자 각각의 확률이 0.1 근처이다. 

   * 정답 ont-hot 인코딩 & cross entropy 출력 

     ```python 
     def _change_ont_hot_label(X, num_category):
         T = np.zeros((X.size, num_category))
         for idx, row in enumerate(T):
             row[X[idx]] = 1
         return T
     
     Y_digit = y_train[:5]
     t = _change_ont_hot_label(Y_digit, 10)
     t     
     >>> array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
            	   [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
                [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
                [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])
     
     print(y_hat[0])
     >>>[0.1242319  0.13073301 0.09501089 0.06100132 0.05761813 0.05451319 0.06424347 0.0794235  0.19205861 0.14116599]
     
     print(t[0])
     >>> [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
     ```

     ```python
     def cross_entropy_error(y, t):
         if y.ndim == 1:
             t = t.reshape(1, t.size)
             y = y.reshape(1, y.size)
             
         if t.size == y.size:
             t = t.argmax(axis=1)
                  
         batch_size = y.shape[0]
         return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size
     
     Loss = cross_entropy_error(y_hat, t)
     Loss
     >>> 2.3092211675206498
     ```

     > 아직은 정답 데이터와 훈련 데이터 예측값의 분포가 비슷하지 않은 것으로 보인다. 



### 경사하강법 

오차를 구했으니 다음 단계는 오차를 줄이는 것이 목표이다. **경사하강법(Gradient Descnt)**은 각 시점의 기울기를 구해 해당 기울기가 가리키는 방향으로 이동하는 방법이다.  정상에서 산 아래로 내려오는 것과 비슷한데, 발 보폭이 너무 크면 산 아래로 내려가지 못하고 골짜기에 빠질 수 도있다. 발 보폭을 **학습률(learning rate)**라고 하고 학습률 만큼의 발걸음으로 기울기를 구해간다. 보폭도 중요하지만, 어디서 출발하느냐도 중요한 문제이다. 이 부분은 **parameter 값을 어떻게 초기화 하느냐**에 달려있다. 

파라미터 W의 변화에 따른 오차(loss)의 변화량을 구하기 위해서는 오차 기울기가 커지는 방향의 **반대 방향**으로 적절한 사이즈의 **learning rate**로 파라미터를 조정해야한다. 

```python 
batch_num = y_hat.shape[0]
dy = (y_hat - t) / batch_num
dy    # softmax값의 출력으로 Loss를 미분한 값
>>> array([[ 0.02484638,  0.0261466 ,  0.01900218,  0.01220026,  0.01152363,
        	-0.18909736,  0.01284869,  0.0158847 ,  0.03841172,  0.0282332 ],
       	   [-0.17502338,  0.02881807,  0.02004818,  0.01084564,  0.01359862,
         	 0.01321532,  0.0123172 ,  0.01587645,  0.03552814,  0.02477575],
       	   [ 0.03103448,  0.02396562,  0.01779764,  0.01196484, -0.1869227 ,
             0.01246838,  0.01252497,  0.01557355,  0.03486518,  0.02672804],
           [ 0.02319132, -0.16860091,  0.02118168,  0.01165467,  0.013361  ,
             0.01187716,  0.01284198,  0.01270892,  0.03651429,  0.02526988],
           [ 0.02787149,  0.02765434,  0.02012748,  0.01263015,  0.01290362,
             0.01448975,  0.01195442,  0.01341485,  0.03126922, -0.17231532]])
```

> dy는 $\frac{\partial Loss}{\partial y}$ 

일단 $dy$가 구해지면 chain-rule로 쉽게 구할 수 있다. 
$$
\frac{∂Loss}{∂W2}=\frac{∂Loss}{\partial y}\; \frac{\partial y}{\partial W2}\\
y={W2}\cdot{z1}+b2\\
\frac{\partial Loss}{\partial W2} = {dy}\cdot{z1}
$$

```python
dW2 = np.dot(z1.T, dy)
db2 = np.sum(dy, axis=0)

def sigmoid_grad(x):
    return (1.0 - sigmoid(x)) * sigmoid(x)

dz1 = np.dot(dy, W2.T)
da1 = sigmoid_grad(a1) * dz1
dW1 = np.dot(X.T, da1)
db1 = np.sum(dz1, axis=0)

learning_rate = 0.1

def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):
    W1 = W1 - learning_rate*dW1
    b1 = b1 - learning_rate*db1
    W2 = W2 - learning_rate*dW2
    b2 = b2 - learning_rate*db2
    return W1, b1, W2, b2
```



### 오차역전파법



오차역전파법은 출력층의 결과와 정답과의 차이를 구한 뒤, 그 오차 값을 각 레이어들을 지나며 역전파해가며 각 노드가 가지고 있는 변수들을 갱신해 나가는 방식이다. 

`affine_layer_backward(X, w, b)`에 대응하여 해당 레이어의 backpropagation 함수를 얻을 수 있게 된다. 

```python 
def affine_layer_backward(dy, cache):
    X, W, b = cache
    dX = np.dot(dy, W.T)
    dW = np.dot(X.T, dy)
    db = np.sum(dy, axis=0)
    return dX, dW, db
```

* 오차역전파법

  ```python
  # 파라미터 초기화
  W1 = weight_init_std * np.random.randn(input_size, hidden_size)
  b1 = np.zeros(hidden_size)
  W2 = weight_init_std * np.random.randn(hidden_size, output_size)
  b2 = np.zeros(output_size)
  
  # Forward Propagation
  a1, cache1 = affine_layer_forward(X, W1, b1)
  z1 = sigmoid(a1)
  a2, cache2 = affine_layer_forward(z1, W2, b2)
  
  # 추론과 오차(Loss) 계산
  y_hat = softmax(a2)
  t = _change_ont_hot_label(Y_digit, 10)   # 정답 One-hot 인코딩
  Loss = cross_entropy_error(y_hat, t)
  
  print(y_hat)
  >>> [[0.06657342 0.03137901 0.13616321 0.10225041 0.0816337  0.06562822
        0.09352708 0.13812927 0.16745731 0.11725838]
       [0.07866398 0.04005858 0.13965209 0.0900657  0.1063248  0.05436466
        0.09034255 0.12559663 0.15721527 0.11771574]
       [0.07614717 0.04432116 0.12069876 0.08081561 0.09613378 0.07040732
        0.09546275 0.14670873 0.16332669 0.10597802]
       [0.09286145 0.04588914 0.11364398 0.1063148  0.09083657 0.07747867
        0.09274047 0.13842424 0.13880091 0.10300976]
       [0.08409881 0.03446255 0.11151767 0.10134067 0.09836175 0.0604527
        0.09955954 0.12404278 0.17724897 0.10891457]]
  
  print(t)
  >>> [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
       [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
       [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
       [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
       [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
  print('Loss: ', Loss)
  >>> Loss:  2.581410413992972
          
  dy = (y_hat - t) / X.shape[0]
  dz1, dW2, db2 = affine_layer_backward(dy, cache2)
  da1 = sigmoid_grad(a1) * dz1
  dX, dW1, db1 = affine_layer_backward(da1, cache1)
  
  # 경사하강법을 통한 파라미터 업데이트    
  learning_rate = 0.1
  W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)
  ```

  

### 모델 학습

```python 
W1 = weight_init_std * np.random.randn(input_size, hidden_size)
b1 = np.zeros(hidden_size)
W2 = weight_init_std * np.random.randn(hidden_size, output_size)
b2 = np.zeros(output_size)

def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):
    a1, cache1 = affine_layer_forward(X, W1, b1)
    z1 = sigmoid(a1)
    a2, cache2 = affine_layer_forward(z1, W2, b2)
    y_hat = softmax(a2)
    t = _change_ont_hot_label(Y, 10)
    Loss = cross_entropy_error(y_hat, t)

    if verbose:
        print('---------')
        print(y_hat)
        print(t)
        print('Loss: ', Loss)
        
    dy = (y_hat - t) / X.shape[0]
    dz1, dW2, db2 = affine_layer_backward(dy, cache2)
    da1 = sigmoid_grad(a1) * dz1
    dX, dW1, db1 = affine_layer_backward(da1, cache1)
    
    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)
    
    return W1, b1, W2, b2, Loss

X = x_train_reshaped[:5]
Y = y_train[:5]

for i in range(5):	# 5번 반복
    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)
    
X = x_train_reshaped[:5]
Y = y_train[:5]

# train_step을 다섯 번 반복
for i in range(5):
    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)

>>> ---------
	[[0.06419478 0.10372244 0.07618257 0.11883948 0.08891903 0.10637488
	  0.1334159  0.07851264 0.04045835 0.18937992]
	 [0.07449389 0.0940668  0.07118463 0.10075996 0.10737369 0.0982242
	  0.11161718 0.1007565  0.04227928 0.19924387]
	 [0.06580917 0.09883376 0.09055533 0.08737671 0.10442344 0.09941183
	  0.13007425 0.08299782 0.03944278 0.20107492]
	 [0.06944383 0.09853943 0.08940389 0.09731005 0.08866553 0.0914094
	  0.1269935  0.10286082 0.03031798 0.20505559]
	 [0.07984328 0.08653356 0.08625258 0.09754508 0.10286938 0.08855203
	  0.13327188 0.09920296 0.03848161 0.18744765]]
	[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
	 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
	 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
	 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
	 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
	Loss:  2.2177358571122836
	---------
    
        생략
        
	---------
	[[0.11472698 0.14770398 0.04477551 0.06776406 0.12005157 0.1873658
	  0.06635127 0.04573088 0.03017011 0.17535984]
	 [0.16016569 0.12294828 0.04143805 0.05604014 0.14129464 0.14559467
	  0.05622198 0.06001074 0.03184721 0.18443861]
	 [0.10820219 0.13248324 0.05723478 0.05102366 0.17385263 0.13805777
	  0.06715756 0.05071538 0.0302692  0.19100361]
	 [0.11130487 0.16375268 0.05763547 0.05850116 0.12118923 0.1365644
	  0.06863437 0.06311266 0.02359167 0.19571349]
	 [0.13850945 0.12035587 0.05283157 0.05523643 0.14467152 0.12929705
	  0.06837882 0.05798143 0.02882294 0.20391492]]
	[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
	 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
	 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
	 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
	 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
	Loss:  1.7310473191817557
```

> Loss 감소



### 정확도(Accuracy) 

앞에서 구한 파라미터들을 가지고 상위 100개에 대한 예측 정확도 구해보기 

```python
def predict(W1, b1, W2, b2, X):
    a1 = np.dot(X, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    y = softmax(a2)

    return y

# 상위 100개 x_train 
X = x_train_reshaped[:100]
Y = y_test[:100]
result = predict(W1, b1, W2, b2, X)
result[0]
>>> array([0.19509273, 0.11432543, 0.04995621, 0.03864222, 0.15233165,
       	   0.16529744, 0.02765668, 0.0435017 , 0.04568711, 0.16750882])

def accuracy(W1, b1, W2, b2, x, y):
    y_hat = predict(W1, b1, W2, b2, x)
    y_hat = np.argmax(y_hat, axis=1)

    accuracy = np.sum(y_hat == y) / float(x.shape[0])
    return accuracy

acc = accuracy(W1, b1, W2, b2, X, Y)

t = _change_one_hot_label(Y, 10)

print(result[0])
>>> [0.19509273 0.11432543 0.04995621 0.03864222 0.15233165 0.16529744
     0.02765668 0.0435017  0.04568711 0.16750882]

print(t[0])
>>>> [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]

print(acc)
>>>> 0.1
```

> 다섯번 학습 사이클을 돌리면 정답률이 10% 수준을 보인다. 



